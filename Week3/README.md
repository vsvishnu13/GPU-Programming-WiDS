# **Week 3 â€” GPU Memory Hierarchy & Performance Optimization**

This week focuses on understanding the GPU memory hierarchy, identifying bottlenecks, and writing kernels based on how GPUs actually move data.

---

##  **Learning Goals**

* Explain the GPU memory hierarchy and its performance implications
* Distinguish between **memory-bound** and **compute-bound** kernels
* Understand **coalesced global memory access**
* Use **shared memory** correctly and avoid common pitfalls
* Identify **bank conflicts** and understand how to mitigate them
* Profile GPU kernels and interpret basic performance metrics

This week is foundational for all serious GPU work, including ML kernels and scientific simulations.

---

## **Required Resources**

### **1. NVIDIA CUDA C++ Programming Guide â€” Memory Model & Performance**

Read the following sections carefully:

* **Section 5.3 â€” Memory Hierarchy**
* **Chapter 8 â€” Performance Guidelines**
  * Section 8.2 (Maximize Utilization) â€” covers occupancy
  * Section 8.3 (Maximize Memory Throughput) â€” covers coalescing and access patterns

ğŸ“„ Link:
[https://docs.nvidia.com/cuda/cuda-c-programming-guide/](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

Focus on:

* Global vs shared vs constant memory
* Latency vs bandwidth
* Memory access patterns and coalescing
* Occupancy 

---

### **2. Mark Harris â€” Coalesced Memory Access (GTC Talk)**

ğŸ¥ Video:
[https://www.nvidia.com/en-us/on-demand/session/gtc24-s62550/](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62550/)

Focus on:

* What "coalesced access" means
* Why strided access is slow
* How warps access memory

---

### **3. GPU Gems 3 â€” Parallel Prefix Sum (Scan)**

Read **Chapter 39**.

ğŸ“„ Link:
[https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda)

Focus on:

* Using shared memory to reduce global memory traffic
* Avoiding bank conflicts
* Structuring parallel algorithms around memory constraints

---

##  **Concepts Covered This Week**

* GPU memory hierarchy:

  * Global memory
  * Shared memory
  * Registers
  * Constant memory 
* Memory latency vs bandwidth
* Coalesced vs non-coalesced access
* Shared memory tiling
* Bank conflicts
* Synchronization (`__syncthreads()`)
* Intro to profiling and timing

---

## **What You Will Build This Week**

You will take simple kernels from Week 2 and **make them faster** by:

* Changing memory access patterns
* Introducing shared memory
* Reducing redundant global loads
* Measuring performance improvements

This is the first time we will see **order-of-magnitude speedups**.

---

## **Week 3 Assignment (Summary)**

### **Task 1 â€” Memory Access Pattern Experiment**

* Implement two versions of a kernel:

  * One with **coalesced** global memory access
  * One with **non-coalesced** (strided) access
* Measure and compare performance
* Explain the difference using warp-level reasoning

---

### **Task 2 â€” Shared Memory Optimization**

* Implement a kernel that:

  * First reads data directly from global memory
  * Then reimplements the same computation using shared memory
* Use `__syncthreads()` correctly
* Compare runtime and memory behavior

---

### **Task 3 â€” CPU Baseline Submission (Deadline)**

This is the **final deadline** to submit your CPU baseline for the workload you identified in Week 1.

You must include:

* `cpu_baseline.py`
* Input sizes
* Measured runtime

This baseline will be used to evaluate GPU speedups in Weeks 4â€“6.

---

## ğŸ“ **Submission Folder**

```
week3/
 â”œâ”€â”€ assignment.pdf
 â”œâ”€â”€ coalesced.cu
 â”œâ”€â”€ non_coalesced.cu
 â”œâ”€â”€ shared_memory.cu
 â”œâ”€â”€ cpu_baseline.py
```
---

## Optional but Highly Recommended (Week 3)

### **1. CUDA Best Practices Guide â€” Memory Optimizations**

ğŸ“„ Link:
[https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

Relevant sections:

* *Memory Optimizations*
* *Occupancy*
* *Performance Guidelines*

---

### **2. Nsight Compute â€” Basic Walkthrough**

Learn how to profile kernels and interpret key GPU metrics.

ğŸ“„ Official Documentation:
[https://docs.nvidia.com/nsight-compute/NsightCompute/index.html](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html)

ğŸ¥ Introductory Tutorial (NVIDIA):
[https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/](https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/)

---

### **3. NVIDIA Blog â€” Shared Memory & Bank Conflicts**

ğŸ“„ Shared Memory Overview:
[https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)

ğŸ“„ Bank Conflicts Explained:
[https://github.com/Kobzol/hardware-effects-gpu/blob/master/bank-conflicts/README.md](https://github.com/Kobzol/hardware-effects-gpu/blob/master/bank-conflicts/README.md)
